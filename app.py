import streamlit as st
import parselmouth
from parselmouth.praat import call
import numpy as np
import pandas as pd
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import os
import platform
import datetime
import io

import sqlite3
import hashlib
import json
import uuid
from pathlib import Path

# --- êµ¬ê¸€ ì‹œíŠ¸ & ì´ë©”ì¼ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---
from google.oauth2 import service_account
import gspread
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.base import MIMEBase
from email import encoders

from sklearn.ensemble import RandomForestClassifier

st.set_page_config(page_title="íŒŒí‚¨ìŠ¨ë³‘ í™˜ì í•˜ìœ„ìœ í˜• ë¶„ë¥˜ í”„ë¡œê·¸ë¨", layout="wide")

# ==========================================
# [ì„¤ì •] êµ¬ê¸€ ì‹œíŠ¸ ì •ë³´ (Secrets)
# ==========================================
HAS_GCP_SECRETS = True
try:
    SHEET_NAME = st.secrets["gcp_info"]["sheet_name"]
except Exception:
    HAS_GCP_SECRETS = False
    SHEET_NAME = None
    # Secretsê°€ ì—†ì–´ë„ ë¶„ì„/DB ì €ì¥ì€ ê°€ëŠ¥í•˜ê²Œ ìœ ì§€
    st.warning("âš ï¸ Secrets(gcp/email) ì„¤ì •ì´ ì—†ì–´ êµ¬ê¸€ì‹œíŠ¸/ì´ë©”ì¼ ì „ì†¡ ê¸°ëŠ¥ì€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤. (DB ì €ì¥ì€ ê°€ëŠ¥)")

# ==========================================
# [ì „ì—­ ì„¤ì •] í°íŠ¸ ë° ë³€ìˆ˜
# ==========================================
FEATS_STEP1 = ['F0', 'Range', 'Intensity', 'SPS', 'VHI_Total', 'VHI_P', 'VHI_F', 'VHI_E']
FEATS_STEP2 = FEATS_STEP1 + ['P_Pitch', 'P_Range', 'P_Loudness', 'P_Rate', 'P_Artic']

def setup_korean_font():
    system_name = platform.system()
    if system_name == 'Windows':
        font_name = "Malgun Gothic"
    elif system_name == 'Darwin':
        font_name = "AppleGothic"
    else:
        font_name = None
    if font_name:
        plt.rc('font', family=font_name)
    plt.rcParams['axes.unicode_minus'] = False

setup_korean_font()

# ==========================================
# [ëª¨ë¸ í•™ìŠµ] training_data.csv ê¸°ë°˜
# ==========================================
@st.cache_resource
def train_models():
    DATA_FILE = "training_data.csv"
    df = None
    if os.path.exists(DATA_FILE) or os.path.exists("training_data.xlsx"):
        loaders = [
            (lambda f: pd.read_excel(f.replace(".csv", ".xlsx")), "excel"),
            (lambda f: pd.read_csv(f, encoding='utf-8'), "utf-8"),
            (lambda f: pd.read_csv(f, encoding='cp949'), "cp949"),
            (lambda f: pd.read_csv(f, encoding='euc-kr'), "euc-kr")
        ]
        target_file = "training_data.xlsx" if os.path.exists("training_data.xlsx") else DATA_FILE
        df_raw = None
        for loader, enc_name in loaders:
            try:
                df_raw = loader(target_file)
                if df_raw is not None and not df_raw.empty: break
            except: 
                continue

        if df_raw is not None:
            try:
                data_list = []
                for _, row in df_raw.iterrows():
                    label = str(row.get('ì§„ë‹¨ê²°ê³¼ (Label)', 'Normal')).strip()
                    if 'normal' in label.lower(): diagnosis, subgroup = "Normal", "Normal"
                    elif 'pd_intensity' in label.lower(): diagnosis, subgroup = "Parkinson", "ê°•ë„ ì§‘ë‹¨"
                    elif 'pd_rate' in label.lower(): diagnosis, subgroup = "Parkinson", "ë§ì†ë„ ì§‘ë‹¨"
                    elif 'pd_articulation' in label.lower(): diagnosis, subgroup = "Parkinson", "ì¡°ìŒ ì§‘ë‹¨"
                    else: 
                        continue

                    raw_total = row.get('VHIì´ì ', 0)
                    raw_p = row.get('VHI_ì‹ ì²´', 0)
                    raw_f = row.get('VHI_ê¸°ëŠ¥', 0)
                    raw_e = row.get('VHI_ì •ì„œ', 0)
                    if raw_total > 40: 
                        vhi_f = (raw_f / 40.0) * 20.0
                        vhi_p = (raw_p / 40.0) * 12.0
                        vhi_e = (raw_e / 40.0) * 8.0
                        vhi_total = vhi_f + vhi_p + vhi_e
                    else:
                        vhi_total, vhi_f, vhi_p, vhi_e = raw_total, raw_f, raw_p, raw_e
                    
                    data_list.append([
                        row.get('F0', 0), row.get('Range', 0), row.get('ê°•ë„(dB)', 0), row.get('SPS', 0),
                        vhi_total, vhi_p, vhi_f, vhi_e,
                        row.get('ìŒë„(ì²­ì§€ê°)', 0), row.get('ìŒë„ë²”ìœ„(ì²­ì§€ê°)', 0), row.get('ê°•ë„(ì²­ì§€ê°)', 0),
                        row.get('ë§ì†ë„(ì²­ì§€ê°)', 0), row.get('ì¡°ìŒì •í™•ë„(ì²­ì§€ê°)', 0),
                        diagnosis, subgroup
                    ])
                df = pd.DataFrame(data_list, columns=FEATS_STEP2 + ['Diagnosis', 'Subgroup'])
                for col in FEATS_STEP2[:4]: 
                    df[col] = df[col].fillna(df[col].mean())
                df[FEATS_STEP1[4:]] = df[FEATS_STEP1[4:]].fillna(0)
            except Exception: 
                df = None

    if df is None: 
        return None, None

    model_step1 = RandomForestClassifier(n_estimators=200, random_state=42)
    model_step1.fit(df[FEATS_STEP1], df['Diagnosis'])

    df_pd = df[df['Diagnosis'] == 'Parkinson'].copy()
    if not df_pd.empty:
        for col in FEATS_STEP2[8:]: 
            df_pd[col] = df_pd[col].fillna(df_pd[col].mean())
        model_step2 = RandomForestClassifier(n_estimators=200, random_state=42)
        model_step2.fit(df_pd[FEATS_STEP2], df_pd['Subgroup'])
    else:
        model_step2 = None

    return model_step1, model_step2

try: 
    model_step1, model_step2 = train_models()
except: 
    model_step1, model_step2 = None, None


# ==========================================
# [DB ì €ì¥] SQLite (ë¬´ë£Œ/ê°„í¸) - ìµëª… ë¡œê·¸ ì €ì¥
#   â€» Streamlit Cloud/ë¬´ë£Œ í˜¸ìŠ¤íŒ…ì—ì„œëŠ” íŒŒì¼ì‹œìŠ¤í…œì´ ì¬ì‹œì‘ ì‹œ ì´ˆê¸°í™”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#     ì§€ì† ì €ì¥ì´ í•„ìš”í•˜ë©´ Postgres ê°™ì€ ì™¸ë¶€ DB(ë¬´ë£Œ í‹°ì–´)ë¥¼ ì—°ê²°í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
# ==========================================
DB_PATH = os.environ.get("PD_TOOL_DB_PATH", "pd_tool.db")

@st.cache_resource
def _init_db():
    db_file = Path(DB_PATH)
    conn = sqlite3.connect(db_file.as_posix(), check_same_thread=False, timeout=30)
    # ë™ì‹œì„± ì™„í™”
    try:
        conn.execute("PRAGMA journal_mode=WAL;")
        conn.execute("PRAGMA synchronous=NORMAL;")
    except Exception:
        pass

    conn.execute("""
    CREATE TABLE IF NOT EXISTS subjects (
        subject_id TEXT PRIMARY KEY,
        created_at TEXT NOT NULL,
        gender TEXT,
        age INTEGER,
        name TEXT
    );
    """)
    conn.execute("""
    CREATE TABLE IF NOT EXISTS analyses (
        analysis_id INTEGER PRIMARY KEY AUTOINCREMENT,
        created_at TEXT NOT NULL,
        subject_id TEXT NOT NULL,
        model_version TEXT,
        step1_pd_cutoff REAL,
        step1_p_pd REAL,
        step1_p_normal REAL,
        step1_pred TEXT,
        final_decision TEXT,
        normal_prob REAL,
        f0 REAL,
        pitch_range REAL,
        intensity_db REAL,
        sps REAL,
        vhi_total REAL,
        vhi_p REAL,
        vhi_f REAL,
        vhi_e REAL,
        p_pitch REAL,
        p_prange REAL,
        p_loud REAL,
        p_rate REAL,
        p_artic REAL,
        wav_sha256 TEXT,
        wav_filename TEXT,
        extra_json TEXT,
        FOREIGN KEY(subject_id) REFERENCES subjects(subject_id)
    );
    """)
    conn.commit()
    return conn

def _subject_id_from_info(name: str, age: int, gender: str) -> str:
    """ê°œì¸ì •ë³´ë¥¼ ê·¸ëŒ€ë¡œ í‚¤ë¡œ ì“°ì§€ ì•Šê¸° ìœ„í•´ salt+hashë¡œ subject_id ìƒì„±"""
    salt = None
    try:
        salt = st.secrets.get("privacy", {}).get("salt", None)
    except Exception:
        salt = None
    if not salt:
        # ë°°í¬ í™˜ê²½ì—ì„œ secretsê°€ ì—†ì„ ë•Œë„ ê³ ì •ëœ í•´ì‹œê°€ ë‚˜ì˜¤ë„ë¡ ì•½í•œ ê¸°ë³¸ salt ì‚¬ìš©
        salt = "PD_TOOL_DEFAULT_SALT"
    raw = f"{salt}|{str(name).strip()}|{str(age).strip()}|{str(gender).strip()}".encode("utf-8", errors="ignore")
    return hashlib.sha256(raw).hexdigest()[:24]

def _sha256_file(path: str) -> str:
    try:
        h = hashlib.sha256()
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(1024 * 1024), b""):
                h.update(chunk)
        return h.hexdigest()
    except Exception:
        return ""

def save_to_sqlite(wav_path: str, patient_info: dict, analysis: dict, diagnosis: dict, model_meta=None):
    """ë¶„ì„ ê²°ê³¼ë¥¼ SQLiteì— ì €ì¥ (êµ¬ê¸€ì‹œíŠ¸/ì´ë©”ì¼ ì—†ì´ë„ ë™ì‘)"""
    conn = _init_db()
    now = datetime.datetime.now().isoformat(timespec="seconds")

    name = str(patient_info.get("name", "")).strip()
    age = patient_info.get("age", None)
    gender = str(patient_info.get("gender", "")).strip()

    try:
        age_int = int(age) if age is not None and str(age).strip() != "" else None
    except Exception:
        age_int = None

    subject_id = _subject_id_from_info(name, age_int if age_int is not None else "", gender)

    # subjects upsert
    conn.execute(
        """INSERT OR IGNORE INTO subjects(subject_id, created_at, gender, age, name)
               VALUES(?, ?, ?, ?, ?);""",
        (subject_id, now, gender, age_int, name if name else None)
    )

    # wav hash
    wav_sha = _sha256_file(wav_path)
    wav_filename = os.path.basename(wav_path) if wav_path else None

    mv = (model_meta or {}).get("model_version", "unknown")
    step1_cutoff = (model_meta or {}).get("step1_pd_cutoff", None)
    step1_p_pd = (model_meta or {}).get("step1_p_pd", None)
    step1_p_norm = (model_meta or {}).get("step1_p_normal", None)
    step1_pred = (model_meta or {}).get("step1_pred", None)

    extra = (model_meta or {}).get("extra", {})
    extra_json = json.dumps(extra, ensure_ascii=False)

    conn.execute(
        """INSERT INTO analyses(
            created_at, subject_id, model_version,
            step1_pd_cutoff, step1_p_pd, step1_p_normal, step1_pred,
            final_decision, normal_prob,
            f0, pitch_range, intensity_db, sps,
            vhi_total, vhi_p, vhi_f, vhi_e,
            p_pitch, p_prange, p_loud, p_rate, p_artic,
            wav_sha256, wav_filename, extra_json
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);""",
        (
            now, subject_id, mv,
            step1_cutoff, step1_p_pd, step1_p_norm, step1_pred,
            str(diagnosis.get("final", "")), float(diagnosis.get("normal_prob", 0.0)),
            float(analysis.get("f0", 0.0)), float(analysis.get("range", 0.0)), float(analysis.get("db", 0.0)), float(analysis.get("sps", 0.0)),
            float(analysis.get("vhi_total", 0.0)), float(analysis.get("vhi_p", 0.0)), float(analysis.get("vhi_f", 0.0)), float(analysis.get("vhi_e", 0.0)),
            float(analysis.get("p_pitch", 0.0)), float(analysis.get("p_prange", 0.0)), float(analysis.get("p_loud", 0.0)), float(analysis.get("p_rate", 0.0)), float(analysis.get("p_artic", 0.0)),
            wav_sha, wav_filename, extra_json
        )
    )
    conn.commit()
    return True, f"SQLite ì €ì¥ ì™„ë£Œ (subject_id={subject_id})"

def db_stats():
    conn = _init_db()
    cur = conn.cursor()
    cur.execute("SELECT COUNT(*) FROM analyses;")
    n_analyses = cur.fetchone()[0]
    cur.execute("SELECT COUNT(*) FROM subjects;")
    n_subjects = cur.fetchone()[0]
    return n_subjects, n_analyses

def fetch_recent_analyses(limit: int = 20):
    conn = _init_db()
    cur = conn.cursor()
    cur.execute(
        """SELECT created_at, subject_id, final_decision, normal_prob, f0, pitch_range, intensity_db, sps
             FROM analyses ORDER BY analysis_id DESC LIMIT ?;""",
        (int(limit),)
    )
    rows = cur.fetchall()
    return pd.DataFrame(rows, columns=["created_at","subject_id","final_decision","normal_prob","f0","pitch_range","intensity_db","sps"])


# ==========================================
# [ì´ë©”ì¼ ì „ì†¡ í•¨ìˆ˜] íŒŒì¼ëª…: ì´ë¦„.wav
# ==========================================
def send_email_and_log_sheet(wav_path, patient_info, analysis, diagnosis):
    if not HAS_GCP_SECRETS:
        return False, "Secretsê°€ ì—†ì–´ êµ¬ê¸€ì‹œíŠ¸/ì´ë©”ì¼ ì „ì†¡ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (SQLite ì €ì¥ì„ ì‚¬ìš©í•˜ì„¸ìš”)"
    try:
        creds = service_account.Credentials.from_service_account_info(
            st.secrets["gcp_service_account"],
            scopes=['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']
        )
        gc = gspread.authorize(creds)
        sh = gc.open(SHEET_NAME)
        worksheet = sh.sheet1
        
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_name = patient_info['name'].replace(" ", "")
        
        # êµ¬ê¸€ ì‹œíŠ¸ìš© íŒŒì¼ëª… (ìƒì„¸ ì •ë³´ í¬í•¨)
        log_filename = f"{safe_name}_{patient_info['age']}_{patient_info['gender']}_{timestamp}.wav"

        row_data = [
            log_filename,
            patient_info['name'], patient_info['age'], patient_info['gender'],
            analysis['f0'], analysis['range'], analysis['db'], analysis['sps'],
            analysis['vhi_total'], analysis['vhi_p'], analysis['vhi_f'], analysis['vhi_e'],
            analysis['p_artic'], analysis['p_pitch'], analysis['p_loud'], analysis['p_rate'], analysis['p_prange'],
            diagnosis['final'], diagnosis['normal_prob']
        ]
        worksheet.append_row(row_data)

        sender = st.secrets["email"]["sender"]
        password = st.secrets["email"]["password"]
        receiver = st.secrets["email"]["receiver"]

        msg = MIMEMultipart()
        msg['From'] = sender
        msg['To'] = receiver
        
        # ì´ë©”ì¼ ì²¨ë¶€ íŒŒì¼ëª…: ì´ë¦„.wav
        email_attach_name = f"{safe_name}.wav"
        msg['Subject'] = f"[PD Data] {email_attach_name}"

        body = f"""
        í™˜ì: {patient_info['name']} ({patient_info['age']}/{patient_info['gender']})
        ì§„ë‹¨: {diagnosis['final']} ({diagnosis['normal_prob']:.1f}%)
        
        * ìŒì„± íŒŒì¼ì´ ì²¨ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤. ({email_attach_name})
        * ìƒì„¸ ìˆ˜ì¹˜ëŠ” êµ¬ê¸€ ì‹œíŠ¸ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.
        """
        msg.attach(MIMEText(body, 'plain'))

        with open(wav_path, "rb") as f:
            part = MIMEBase("audio", "wav")
            part.set_payload(f.read())
        
        encoders.encode_base64(part)
        part.add_header("Content-Disposition", f'attachment; filename="{email_attach_name}"')
        msg.attach(part)

        server = smtplib.SMTP(st.secrets["email"]["smtp_server"], st.secrets["email"]["smtp_port"])
        server.starttls()
        server.login(sender, password)
        server.sendmail(sender, receiver, msg.as_string())
        server.quit()

        return True, "êµ¬ê¸€ ì‹œíŠ¸ ê¸°ë¡ + ì´ë©”ì¼ ì „ì†¡ ì„±ê³µ"
    except Exception as e:
        return False, str(e)

# ============================
# ì´í•˜ ì›ë³¸ ë¶„ì„/UI ë¡œì§ (ê·¸ëŒ€ë¡œ)
# ============================

def plot_pitch_contour_plotly(file_path, f0_min=70, f0_max=500):
    sound = parselmouth.Sound(file_path)
    pitch = sound.to_pitch(time_step=0.01, pitch_floor=f0_min, pitch_ceiling=f0_max)
    f0_values = pitch.selected_array['frequency']
    f0_values = f0_values[f0_values != 0]
    times = pitch.xs()

    if len(f0_values) == 0:
        return None, 0, 0, sound.duration
    
    f0_mean = np.mean(f0_values)
    f0_range = np.max(f0_values) - np.min(f0_values)

    fig = go.Figure()
    fig.add_trace(go.Scatter(x=times, y=pitch.selected_array['frequency'], mode='lines', name="F0"))
    fig.update_layout(
        title="Pitch Contour",
        xaxis_title="Time (s)",
        yaxis_title="Frequency (Hz)",
        yaxis=dict(range=[0, f0_max]),
        height=250
    )
    return fig, f0_mean, f0_range, sound.duration

def auto_detect_smr_events(file_path):
    try:
        sound = parselmouth.Sound(file_path)
        pitch = sound.to_pitch(time_step=0.01, pitch_floor=60, pitch_ceiling=600)
        values = pitch.selected_array['frequency']
        times = pitch.xs()

        nz = values[values > 0]
        if len(nz) < 5:
            return [], 0
        
        med = np.median(nz)
        low_th = med / 1.9
        hi_th = med * 1.9
        values_f = values.copy()
        values_f[(values_f < low_th) | (values_f > hi_th)] = 0

        changes = np.abs(np.diff(values_f))
        jump_th = np.percentile(changes[changes > 0], 95) if np.any(changes > 0) else 0
        peaks = np.where(changes > jump_th)[0]

        candidates = []
        for p_idx in peaks:
            time_point = times[p_idx]
            v_int = values_f[p_idx]
            start_search = max(0, p_idx - 20)
            end_search = min(len(values_f), p_idx + 20)
            local_max = np.max(values_f[start_search:end_search])
            depth = local_max - v_int
            candidates.append({"time": time_point, "depth": depth})
        candidates.sort(key=lambda x: x['time'])
        return candidates, len(candidates)
    except:
        return [], 0

def run_analysis_logic(file_path):
    try:
        fig, f0, rng, dur = plot_pitch_contour_plotly(file_path, 70, 500)
        sound = parselmouth.Sound(file_path)
        intensity = sound.to_intensity()
        mean_db = call(intensity, "Get mean", 0, 0, "energy")
        sps = st.session_state.user_syllables / dur if dur > 0 else 0
        smr_events, smr_count = auto_detect_smr_events(file_path)
        
        st.session_state.update({
            'f0_mean': f0, 'pitch_range': rng, 'mean_db': mean_db, 
            'sps': sps, 'duration': dur, 'fig_plotly': fig, 
            'smr_events': smr_events, 'smr_count': smr_count,
            'is_analyzed': True, 'is_saved': False
        })
        return True
    except Exception as e:
        st.error(f"ë¶„ì„ ì˜¤ë¥˜: {e}"); return False

def generate_interpretation(prob_normal, db, sps, range_val, artic, vhi, vhi_e):
    positives, negatives = [], []
    if vhi < 15: positives.append(f"í™˜ì ë³¸ì¸ì˜ ì£¼ê´€ì  ë¶ˆí¸í•¨(VHI {vhi}ì )ì´ ë‚®ì•„, ì¼ìƒ ëŒ€í™”ì— ì‹¬ë¦¬ì /ê¸°ëŠ¥ì  ë¶€ë‹´ì´ ì ì€ ìƒíƒœì…ë‹ˆë‹¤.")
    if range_val >= 100: positives.append(f"ìŒë„ ë²”ìœ„ê°€ {range_val:.1f}Hzë¡œ ë„“ê²Œ ë‚˜íƒ€ë‚˜, ëª©ì†Œë¦¬ì— ìƒë™ê°ì´ ìˆê³  ì–µì–‘ì˜ ë³€í™”ê°€ ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤.")
    if artic >= 75: positives.append(f"ì²­ì§€ê°ì  ì¡°ìŒ ì •í™•ë„ê°€ {artic}ì ìœ¼ë¡œ ì–‘í˜¸í•˜ì—¬, ìƒëŒ€ë°©ì´ ë§ì„ ì•Œì•„ë“£ê¸°ì— ëª…ë£Œí•©ë‹ˆë‹¤.")
    if db >= 60: positives.append(f"í‰ê·  ë°œí™” ê°•ë„ê°€ {db:.1f}dBë¡œ ì¶©ë¶„í•˜ì—¬, ëª©ì†Œë¦¬ê°€ ë¹„êµì  ë˜ë ·í•˜ê²Œ ì „ë‹¬ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    if vhi_e < 5: positives.append(f"ì •ì„œì  ì˜í–¥(VHI-E {vhi_e}ì )ì´ ë‚®ì•„, ëª©ì†Œë¦¬ ë¬¸ì œë¡œ ì¸í•œ ìŠ¤íŠ¸ë ˆìŠ¤/ë¶ˆì•ˆì´ ìƒëŒ€ì ìœ¼ë¡œ ì ì€ í¸ì…ë‹ˆë‹¤.")

    if vhi >= 20: negatives.append(f"ì£¼ê´€ì  ë¶ˆí¸í•¨(VHI {vhi}ì )ì´ ë†’ì•„, ìŒì„± ë¬¸ì œë¡œ ì¼ìƒìƒí™œì—ì„œ ë¶ˆí¸/ë¶€ë‹´ì„ ëŠë‚„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
    if range_val < 70: negatives.append(f"ìŒë„ ë²”ìœ„ê°€ {range_val:.1f}Hzë¡œ ì¢ì•„, ì–µì–‘ ë³€í™”ê°€ ì œí•œë˜ì–´ ë‹¨ì¡°ë¡­ê²Œ ë“¤ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    if artic < 65: negatives.append(f"ì²­ì§€ê°ì  ì¡°ìŒ ì •í™•ë„ê°€ {artic}ì ìœ¼ë¡œ ë‚®ì•„, ë§ì†Œë¦¬ê°€ ë­‰ê°œì ¸ ë“¤ë¦´ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
    if db < 55: negatives.append(f"í‰ê·  ë°œí™” ê°•ë„ê°€ {db:.1f}dBë¡œ ë‚®ì•„, ëª©ì†Œë¦¬ê°€ ì‘ê±°ë‚˜ ì•½í•˜ê²Œ ì „ë‹¬ë  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
    if sps > 5.5: negatives.append(f"ë§ì†ë„(SPS)ê°€ {sps:.2f}ë¡œ ë¹ ë¥¸ í¸ì´ë¼, ë§ì´ ê¸‰í•˜ê²Œ ë“¤ë¦¬ê±°ë‚˜ ëª…ë£Œë„ê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    if vhi_e >= 7: negatives.append(f"ì •ì„œì  ì˜í–¥(VHI-E {vhi_e}ì )ì´ ë†’ì•„, ìŒì„± ë¬¸ì œë¡œ ìŠ¤íŠ¸ë ˆìŠ¤/ë¶ˆì•ˆì´ ë™ë°˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    return positives, negatives

# ==========================================
# UI
# ==========================================
st.title("ğŸ§  íŒŒí‚¨ìŠ¨ë³‘ í™˜ì í•˜ìœ„ìœ í˜• ë¶„ë¥˜ í”„ë¡œê·¸ë¨")
st.write("ìŒì„± íŒŒì¼(.wav)ì„ ì—…ë¡œë“œí•˜ê³  ê°„ë‹¨í•œ ì…ë ¥ í›„ ë¶„ì„ì„ ì§„í–‰í•˜ì„¸ìš”.")

if 'user_syllables' not in st.session_state:
    st.session_state.user_syllables = 0
if 'is_analyzed' not in st.session_state:
    st.session_state.is_analyzed = False
if 'is_saved' not in st.session_state:
    st.session_state.is_saved = False

uploaded_file = st.file_uploader("ğŸ¤ ìŒì„± íŒŒì¼ ì—…ë¡œë“œ (.wav)", type=["wav"])
if uploaded_file:
    temp_dir = "temp_uploads"
    os.makedirs(temp_dir, exist_ok=True)
    file_path = os.path.join(temp_dir, uploaded_file.name)
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    st.session_state.current_wav_path = file_path

    st.audio(uploaded_file, format="audio/wav")

    st.markdown("### ğŸ§¾ ê¸°ë³¸ ì •ë³´ ì…ë ¥")
    subject_name = st.text_input("ì´ë¦„", value="")
    subject_age = st.number_input("ë‚˜ì´", min_value=1, max_value=120, value=60)
    subject_gender = st.selectbox("ì„±ë³„", ["M", "F"])
    user_syllables = st.number_input("ë°œí™”í•œ ìŒì ˆ ìˆ˜(ëŒ€ëµ)", min_value=1, max_value=500, value=40)
    st.session_state.user_syllables = user_syllables

    st.markdown("---")
    if st.button("ğŸ“ˆ ìŒì„± ë¶„ì„ ì‹¤í–‰"):
        ok = run_analysis_logic(file_path)
        if ok:
            st.success("âœ… ë¶„ì„ ì™„ë£Œ! ì•„ë˜ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    if st.session_state.get('is_analyzed'):
        st.markdown("### ğŸ“Œ ë¶„ì„ ê²°ê³¼")
        st.plotly_chart(st.session_state.fig_plotly, use_container_width=True)

        # ê¸°ë³¸ ê°’ë“¤
        f0 = st.session_state['f0_mean']
        rng = st.session_state['pitch_range']
        mean_db = st.session_state['mean_db']
        sps = st.session_state['sps']
        dur = st.session_state['duration']
        smr_count = st.session_state.get('smr_count', 0)

        st.write(f"- í‰ê·  F0: {f0:.1f} Hz")
        st.write(f"- ìŒë„ ë²”ìœ„: {rng:.1f} Hz")
        st.write(f"- í‰ê·  ê°•ë„: {mean_db:.1f} dB")
        st.write(f"- SPS(ì´ˆë‹¹ ìŒì ˆ ìˆ˜): {sps:.2f}")
        st.write(f"- ë°œí™” ê¸¸ì´: {dur:.2f} s")
        st.write(f"- SMR ì´ë²¤íŠ¸(ìë™ íƒì§€): {smr_count}íšŒ")

        st.markdown("### ğŸ§¾ VHI ì…ë ¥")
        vhi_total = st.number_input("VHI ì´ì ", 0, 120, 0)
        vhi_p = st.number_input("VHI-ì‹ ì²´", 0, 40, 0)
        vhi_f = st.number_input("VHI-ê¸°ëŠ¥", 0, 40, 0)
        vhi_e = st.number_input("VHI-ì •ì„œ", 0, 40, 0)

        st.markdown("### ğŸ‘‚ ì²­ì§€ê° í‰ê°€ ì…ë ¥")
        p_artic = st.number_input("ì¡°ìŒì •í™•ë„(ì²­ì§€ê°)", 0, 100, 75)
        p_pitch = st.number_input("ìŒë„(ì²­ì§€ê°)", 0, 100, 50)
        p_prange = st.number_input("ìŒë„ë²”ìœ„(ì²­ì§€ê°)", 0, 100, 50)
        p_loud = st.number_input("ê°•ë„(ì²­ì§€ê°)", 0, 100, 50)
        p_rate = st.number_input("ë§ì†ë„(ì²­ì§€ê°)", 0, 100, 50)

        st.markdown("---")
        if st.button("ğŸš€ ì§„ë‹¨ ê²°ê³¼ í™•ì¸"):
            if model_step1:
                range_adj = rng
                final_db = mean_db
                final_sps = sps

                input_1 = pd.DataFrame([[
                    f0, range_adj, final_db, final_sps,
                    vhi_total, vhi_p, vhi_f, vhi_e
                ]], columns=FEATS_STEP1)

                pred_1 = model_step1.predict(input_1)[0]
                proba_1 = model_step1.predict_proba(input_1)[0]
                classes_1 = list(model_step1.classes_)
                prob_normal = float(proba_1[classes_1.index("Normal")]) * 100 if "Normal" in classes_1 else 0.0

                if pred_1 == "Normal":
                    st.success(f"ğŸŸ¢ ì •ìƒ ìŒì„± (Normal) - {prob_normal:.1f}%")
                    final_decision = "Normal"
                else:
                    st.error(f"ğŸ”´ íŒŒí‚¨ìŠ¨ ê°€ëŠ¥ì„± (PD) - {100 - prob_normal:.1f}%")
                    if model_step2:
                        input_2 = pd.DataFrame([[
                            f0, range_adj, final_db, final_sps,
                            vhi_total, vhi_p, vhi_f, vhi_e,
                            p_pitch, p_prange, p_loud, p_rate, p_artic
                        ]], columns=FEATS_STEP2)

                        pred_2 = model_step2.predict(input_2)[0]
                        final_decision = pred_2
                        st.info(f"â¡ï¸ PD í•˜ìœ„ ì§‘ë‹¨ ì˜ˆì¸¡: {pred_2}")
                    else:
                        final_decision = "Parkinson"

                pos, neg = generate_interpretation(prob_normal, final_db, final_sps, range_adj, p_artic, vhi_total, vhi_e)
                st.markdown("### ğŸ§¾ í•´ì„(ìš”ì•½)")
                st.markdown(f"**1. ì •ìƒì¼ í™•ë¥ ì´ {prob_normal:.1f}%ë¡œ ë‚˜ì˜¨ ì´ìœ  (ê¸ì • ìš”ì¸):**")
                if pos:
                    for p in pos:
                        st.markdown(f"- âœ… {p}")
                else:
                    st.markdown("- íŠ¹ë³„í•œ ê°•ì  ìš”ì¸ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

                st.markdown(f"**2. íŒŒí‚¨ìŠ¨(PD) ê°€ëŠ¥ì„±ì´ {100-prob_normal:.1f}% ì¡´ì¬í•˜ëŠ” ì´ìœ  (ìœ„í—˜ ìš”ì¸):**")
                if neg:
                    for n in neg:
                        st.markdown(f"- âš ï¸ {n}")
                else:
                    st.markdown("- íŠ¹ë³„í•œ ìœ„í—˜ ìš”ì¸ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

                st.session_state.save_ready_data = {
                    'wav_path': st.session_state.current_wav_path,
                    'patient': {'name': subject_name, 'age': subject_age, 'gender': subject_gender},
                    'analysis': {
                        'f0': f0, 'range': range_adj, 'db': final_db, 'sps': final_sps,
                        'vhi_total': vhi_total, 'vhi_p': vhi_p, 'vhi_f': vhi_f, 'vhi_e': vhi_e,
                        'p_artic': p_artic, 'p_pitch': p_pitch, 'p_loud': p_loud, 'p_rate': p_rate, 'p_prange': p_prange
                    },
                    'diagnosis': {'final': final_decision, 'normal_prob': prob_normal}
                }
            else:
                st.error("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")

# ì „ì†¡ ë²„íŠ¼
st.markdown("---")
if st.button("â˜ï¸ ë°ì´í„° ì „ì†¡ (ë©”ì¼+ì‹œíŠ¸)", type="primary"):
    if 'save_ready_data' not in st.session_state:
        st.error("ğŸš¨ ì „ì†¡í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € [ğŸš€ ì§„ë‹¨ ê²°ê³¼ í™•ì¸]ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!")
    elif st.session_state.get('is_saved'):
        st.warning("ì´ë¯¸ ì „ì†¡ëœ ë°ì´í„°ì…ë‹ˆë‹¤.")
    else:
        with st.spinner("êµ¬ê¸€ ì‹œíŠ¸ ê¸°ë¡ ë° ì´ë©”ì¼ ì „ì†¡ ì¤‘..."):
            success, msg = send_email_and_log_sheet(
                st.session_state.save_ready_data['wav_path'], 
                st.session_state.save_ready_data['patient'], 
                st.session_state.save_ready_data['analysis'], 
                st.session_state.save_ready_data['diagnosis']
            )
        if success:
            st.session_state.is_saved = True
            st.success(f"âœ… ì²˜ë¦¬ ì™„ë£Œ! {msg}")
        else:
            st.error(f"âŒ ì „ì†¡ ì‹¤íŒ¨: {msg}")


# ==========================================
# [ì¶”ê°€] DB ì €ì¥ ë²„íŠ¼ (SQLite)
# ==========================================
if st.button("ğŸ—„ï¸ DB ì €ì¥ (SQLite)", type="secondary"):
    if 'save_ready_data' not in st.session_state:
        st.error("ğŸš¨ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € [ğŸš€ ì§„ë‹¨ ê²°ê³¼ í™•ì¸]ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!")
    else:
        with st.spinner("SQLite ì €ì¥ ì¤‘..."):
            ok, msg = save_to_sqlite(
                st.session_state.save_ready_data['wav_path'],
                st.session_state.save_ready_data['patient'],
                st.session_state.save_ready_data['analysis'],
                st.session_state.save_ready_data['diagnosis'],
                model_meta={
                    "model_version": "v1.0",
                    "step1_pd_cutoff": None,
                    "step1_p_pd": None,
                    "step1_p_normal": st.session_state.save_ready_data['diagnosis'].get('normal_prob', 0.0)/100.0,
                    "step1_pred": st.session_state.save_ready_data['diagnosis'].get('final', None),
                    "extra": {"note": "saved_from_streamlit"}
                }
            )
        if ok:
            st.success(f"âœ… {msg}")
        else:
            st.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {msg}")

with st.expander("ğŸ—„ï¸ DB í˜„í™© / ìµœê·¼ ì €ì¥ ê¸°ë¡"):
    try:
        n_subj, n_ana = db_stats()
        st.write(f"- Subjects: **{n_subj}**")
        st.write(f"- Analyses: **{n_ana}**")

        df_recent = fetch_recent_analyses(limit=20)
        if not df_recent.empty:
            st.dataframe(df_recent, use_container_width=True)
            csv_bytes = df_recent.to_csv(index=False).encode("utf-8-sig")
            st.download_button("ìµœê·¼ 20ê±´ CSV ë‹¤ìš´ë¡œë“œ", data=csv_bytes, file_name="recent_analyses.csv", mime="text/csv")
        else:
            st.write("ìµœê·¼ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")

        # DB íŒŒì¼ ë‹¤ìš´ë¡œë“œ(ì„œë²„ íŒŒì¼ ì ‘ê·¼ì´ ê°€ëŠ¥í•œ í™˜ê²½ì—ì„œë§Œ ì˜ë¯¸)
        try:
            db_file = Path(DB_PATH)
            if db_file.exists():
                st.download_button(
                    "DB íŒŒì¼ ë‹¤ìš´ë¡œë“œ (pd_tool.db)",
                    data=db_file.read_bytes(),
                    file_name=db_file.name,
                    mime="application/octet-stream"
                )
        except Exception:
            pass
    except Exception as e:
        st.write(f"DB ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}")
